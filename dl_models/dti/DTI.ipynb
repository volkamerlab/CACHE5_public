{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07bad267-9998-4176-9c17-33c02c2dbf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Tuple, Dict, Set, Callable\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.functional import Tensor\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.typing import Adj\n",
    "from pytorch_lightning import seed_everything, Trainer, LightningDataModule, LightningModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichModelSummary, RichProgressBar\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from torchmetrics import MetricCollection, Accuracy, AUROC, MatthewsCorrCoef, MeanSquaredError, MeanAbsoluteError, ExplainedVariance\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53962b00-8627-48d7-82e5-fe5088b2b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = {\n",
    "    \"L01\": \"C1CCCCC1\",\n",
    "    \"L02\": \"O1CCOCC1\",\n",
    "    \"L03\": \"C1CCCC2C1CCCC2\",\n",
    "    \"L04\": \"C1CCCCC1C2CCCCC2\",\n",
    "    \"L05\": \"c1ccccc1-c2ccccc2\",\n",
    "    \"L06\": \"CC(C)(C)OC(=O)N1CCC[C@H]1C(=O)O\",\n",
    "    \"L07\": \"O=Cc1ccc(O)c(OC)c1\",\n",
    "    \"L08\": \"CC(=O)NCCC1=CNc2c1cc(OC)cc2\",\n",
    "    \"L09\": \"CCc(c1)ccc2[n+]1ccc3c2[nH]c4c3cccc4\",\n",
    "    \"L10\": \"CCC[C@@H](O)CC\\C=C\\C=C\\C#CC#C\\C=C\\CO\",\n",
    "}\n",
    "aa_seqs = {f\"G{i + 1:02d}\": ''.join(random.choices(\"ACDEFGHIKLMNPQRSTVWY\", k=40)) for i in range(10)}\n",
    "\n",
    "p_values_range = (5, 10)\n",
    "classes = [0, 1]\n",
    "labels = []\n",
    "records = {\n",
    "    \"LIG_ID\": list(mols.keys()),\n",
    "    \"GENE_ID\": list(aa_seqs.keys()),\n",
    "    \"acname\": [],\n",
    "    \"acvalue_uM\": [],\n",
    "    \"class\": [],\n",
    "}\n",
    "for _ in range(10):\n",
    "    records[\"acvalue_uM\"].append(round(random.uniform(*p_values_range), 2))\n",
    "    records[\"class\"].append(int(records[\"acvalue_uM\"][-1] < 6))\n",
    "    records[\"acname\"].append(\"pIC50\" if random.choice([True, False]) else \"pKi\")\n",
    "\n",
    "data = pd.DataFrame(records)\n",
    "with open(dummy_path := Path(\"data\") / \"dummy.pkl\", \"wb\") as f:\n",
    "    pickle.dump((data, mols, aa_seqs), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e49aa2a-f7b9-4729-844c-3a997f730253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolEmbed(LightningModule):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128, output_dim: int = 128, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.inp = GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "            )\n",
    "        )\n",
    "        mid_layers = [\n",
    "            GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.PReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                )\n",
    "            )\n",
    "            for _ in range(num_layers - 2)\n",
    "        ]\n",
    "        self.mid_layers = nn.ModuleList(mid_layers)\n",
    "        self.out = GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.PReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim),\n",
    "                nn.BatchNorm1d(output_dim),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, data, **kwargs) -> Tensor:\n",
    "        \"\"\"Forward the data through the GNN module\"\"\"\n",
    "        x = self.inp(data.x, data.edge_index)\n",
    "        for module in self.mid_layers:\n",
    "            x = module(x, data.edge_index)\n",
    "        x = self.out(x, data.edge_index)\n",
    "        pool = global_mean_pool(x, data[\"batch\"])\n",
    "        return F.normalize(pool, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df52007-0ee2-4932-baf9-96b1906ad71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CACHE_DTI(LightningModule):\n",
    "    def __init__(self, batch_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.drug_embedder = MolEmbed(input_dim=9, hidden_dim=128, output_dim=128, num_layers=3)\n",
    "        self.prot_embedder = nn.Sequential(nn.Linear(768, 128), nn.PReLU())\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 64), \n",
    "            nn.PReLU(), \n",
    "            nn.Dropout(0.2), \n",
    "            nn.Linear(64, 3), \n",
    "            nn.PReLU(), \n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.reg_criterion = nn.MSELoss()\n",
    "        self.class_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.metrics = self._set_metrics()\n",
    "        self.automatic_optimization = False\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        for splits in self.metrics.values():\n",
    "            for mc in splits.values():\n",
    "                mc.to(device)\n",
    "\n",
    "    def _set_metrics(self):\n",
    "        class_metrics = MetricCollection([\n",
    "            Accuracy(task=\"binary\"),\n",
    "            AUROC(task=\"binary\"),\n",
    "            MatthewsCorrCoef(task=\"binary\"),\n",
    "        ])\n",
    "        reg_metrics = MetricCollection([\n",
    "            MeanAbsoluteError(),\n",
    "            MeanSquaredError(),\n",
    "            ExplainedVariance(),\n",
    "        ])\n",
    "        return {\n",
    "            \"class\": {\n",
    "                \"train\": class_metrics.clone(\"class/train/\"),\n",
    "                \"val\": class_metrics.clone(\"class/val/\"),\n",
    "                \"test\": class_metrics.clone(\"class/test/\"),\n",
    "            }, \n",
    "            \"reg\": {\n",
    "                \"train\": reg_metrics.clone(\"reg/train/\"),\n",
    "                \"val\": reg_metrics.clone(\"reg/val/\"),\n",
    "                \"test\": reg_metrics.clone(\"reg/test/\"),\n",
    "            }\n",
    "        }\n",
    "     \n",
    "    def forward(self, data):\n",
    "        drug_embed = self.drug_embedder(data)\n",
    "        prot_embed = self.prot_embedder(data[\"t5\"])\n",
    "        comb_embed = torch.cat((drug_embed, prot_embed), dim=1)\n",
    "        pred = self.head(comb_embed)\n",
    "        mask = data[\"y\"][:, :2] == -1\n",
    "        pred[:, :2][mask] = data[\"y\"][:, :2][mask]\n",
    "        return {\n",
    "            \"reg_pred\": pred[:, :-1],\n",
    "            \"reg_labels\": data[\"y\"][:, :-1],\n",
    "            \"class_pred\": pred[:, -1],\n",
    "            \"class_labels\": data[\"y\"][:, -1].float(),\n",
    "        }\n",
    "\n",
    "    def shared_step(self, data):\n",
    "        fwd_dict = self.forward(data)\n",
    "        fwd_dict[\"reg_loss\"] = self.reg_criterion(fwd_dict[\"reg_pred\"], fwd_dict[\"reg_labels\"])\n",
    "        fwd_dict[\"class_loss\"] = self.class_criterion(fwd_dict[\"class_pred\"], fwd_dict[\"class_labels\"])\n",
    "        return fwd_dict\n",
    "\n",
    "    def update(self, fwd, stage):\n",
    "        self.metrics[\"reg\"][stage].update(fwd[\"reg_pred\"].contiguous(), fwd[\"reg_labels\"].contiguous())\n",
    "        # self.metrics[\"class\"][stage].update(fwd[\"class_pred\"], fwd[\"class_labels\"])\n",
    "        self.log(f\"{stage}/reg/loss\", fwd[\"reg_loss\"], batch_size=self.batch_size)\n",
    "        # self.log(f\"{stage}/class/loss\", fwd[\"class_loss\"], batch_size=self.batch_size)\n",
    "\n",
    "    def log_histograms(self):\n",
    "        \"\"\"Logs the histograms of all the available parameters.\"\"\"\n",
    "        if self.logger:\n",
    "            for name, param in self.named_parameters():\n",
    "                self.logger.experiment.add_histogram(name, param, self.current_epoch)\n",
    "\n",
    "    def training_step(self, data, data_idx):\n",
    "        fwd = self.shared_step(data)\n",
    "        self.update(fwd, \"train\")\n",
    "\n",
    "        # Do backpropagation\n",
    "        self.optimizers().zero_grad()\n",
    "        self.manual_backward(fwd[\"reg_loss\"])  # , retain_graph=True)\n",
    "        # self.manual_backward(fwd[\"class_loss\"])\n",
    "        self.optimizers().step()\n",
    "        \n",
    "        return fwd\n",
    "\n",
    "    def validation_step(self, data, data_idx):\n",
    "        print(\"validation step\")\n",
    "        fwd = self.shared_step(data)\n",
    "        self.update(fwd, \"val\")\n",
    "        print(self.metrics[\"reg\"][\"val\"])\n",
    "        return fwd\n",
    "\n",
    "    def test_step(self, data, data_idx):\n",
    "        fwd = self.shared_step(data)\n",
    "        seld.update(fwd, \"test\")\n",
    "        return fwd\n",
    "        \n",
    "    def log_all(self, metrics: dict, hparams: bool = False):\n",
    "        \"\"\"Log all metrics.\"\"\"\n",
    "        print(metrics)\n",
    "        if self.logger:\n",
    "            for k, v in metrics.items():\n",
    "                print(\"Logging\", k, \":\", v)\n",
    "                self.log(k, v, self.current_epoch)\n",
    "\n",
    "    def shared_end(self, stage):\n",
    "        for task in [\"reg\"]:  # , \"class\"]:\n",
    "            print(\"Logging for\", task)\n",
    "            metrics = self.metrics[task][stage].compute()\n",
    "            self.metrics[task][stage].reset()\n",
    "            self.log_all(metrics)\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        self.log_histograms()\n",
    "        print(\"Logging training end\")\n",
    "        self.shared_end(\"train\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        print(\"Logging validation end\")\n",
    "        self.shared_end(\"val\")\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        print(\"Logging testing end\")\n",
    "        self.shared_end(\"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f7d2c6-f91e-4eec-aa89-9c4c8cfe84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProstT5(aa_seqs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/ProstT5\", do_lower_case=False)\n",
    "    prostt5 = T5EncoderModel.from_pretrained(\"Rostlab/ProstT5\").to(device)\n",
    "    prostt5.full() if device == \"cpu\" else prostt5.half()\n",
    "    \n",
    "    seqs = [\" \".join([\"<fold2AA>\"] + list(re.sub(\"[OUZ]\", \"X\", seq))) for seq in aa_seqs]\n",
    "    max_len = max(len(s) for s in aa_seqs)\n",
    "    encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        aaseq_embed = prostt5(encoding.input_ids, attention_mask=encoding.attention_mask)\n",
    "    return dict(zip(aa_seqs, aaseq_embed.last_hidden_state[:, 1 : max_len + 1].mean(dim=1).cpu()))\n",
    "\n",
    "\n",
    "def ProtT5(aa_seqs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_base_mt_uniref50\", do_lower_case=False)\n",
    "    prott5 = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_base_mt_uniref50\").to(device)\n",
    "    seqs = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in aa_seqs]\n",
    "    \n",
    "    ids = tokenizer(aa_seqs, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "    max_len = max(len(s) for s in aa_seqs)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding_repr = prott5(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    return dict(zip(aa_seqs, embedding_repr.last_hidden_state[:, 1 : max_len + 1].mean(dim=1).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a16e4078-c447-4d92-ac68-54ac8b710658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(root, filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        inter, lig_map, tar_map = pickle.load(f)\n",
    "    if (t5_path := Path(root) / f\"prostt5_{filename.stem}.pkl\").exists():\n",
    "        with open(t5_path, \"rb\") as f:\n",
    "            t5 = pickle.load(f)\n",
    "    else:\n",
    "        t5 = ProtT5(list(tar_map.values()))\n",
    "        with open(t5_path, \"wb\") as f:\n",
    "            pickle.dump(t5, f)\n",
    "    lig_data = {k: torch_geometric.utils.from_smiles(v) for k, v in lig_map.items()}\n",
    "    return inter, lig_data, tar_map, t5\n",
    "\n",
    "\n",
    "def row2data(row, lig_data, tar_map, t5):\n",
    "    d = lig_data[row[\"LIG_ID\"]].clone()\n",
    "    d[\"t5\"] = torch.tensor(t5[tar_map[row[\"GENE_ID\"]]]).reshape(1, -1)\n",
    "    if row[\"acname\"] == \"pKi\":\n",
    "        d[\"y\"] = [row[\"acvalue_uM\"], -1, row[\"class\"]]\n",
    "    elif row[\"acname\"] == \"pIC50\":\n",
    "        d[\"y\"] = [-1, row[\"acvalue_uM\"], row[\"class\"]]\n",
    "    d[\"y\"] = torch.tensor(d[\"y\"]).reshape(1, -1)\n",
    "    return d\n",
    "\n",
    "\n",
    "class BaseDataset(InMemoryDataset):\n",
    "    def __init__(self, filename, path_idx: int = 0):\n",
    "        self.filename = Path(filename)\n",
    "        super().__init__(Path(\"data\"))\n",
    "        self.data, self.slices = torch.load(self.processed_paths[path_idx])\n",
    "\n",
    "    @property\n",
    "    def processed_paths(self):\n",
    "        return [self.root / f for f in self.processed_file_names]\n",
    "\n",
    "    def process_(self, data, path_idx: int = 0):\n",
    "        data, slices = self.collate(data)\n",
    "        torch.save((data, slices), self.processed_paths[path_idx])\n",
    "\n",
    "\n",
    "class PretrainDataset(BaseDataset):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__(filename)\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.filename.stem + \".pt\"]\n",
    "\n",
    "    def process(self):\n",
    "        inter, lig_data, tar_map, t5 = prepare(self.root, self.filename)\n",
    "        data = []\n",
    "        for _, row in inter.iterrows():\n",
    "            try:\n",
    "                data.append(row2data(row, lig_data, tar_map, t5))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        self.process_(data)\n",
    "\n",
    "\n",
    "class MCHR1Dataset(BaseDataset):\n",
    "    splits = {\"train\": 0, \"val\": 1, \"ensemble\": 2, \"test\": 3}\n",
    "    \n",
    "    def __init__(self, filename, split):\n",
    "        super().__init__(filename, self.splits[split])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [k + \".pt\" for k in [\"train\", \"val\", \"ensemble\", \"test\"]]\n",
    "    \n",
    "    def process(self):\n",
    "        inter, lig_data, tar_map, t5 = prepare(self.root, self.filename)\n",
    "        data = {k: [] for k in self.splits.keys()}\n",
    "        for _, row in inter.iterrows():\n",
    "            try:\n",
    "                data[row[\"split\"]].append(row2data(row, lig_data, tar_map, t5))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        for split in self.splits.keys():\n",
    "            self.process_(data[split], self.splits[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e76ca6b8-2835-4ced-a92f-5169b012488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=min(self.batch_size, len(self.train)), shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dl = DataLoader(self.val, batch_size=min(self.batch_size, len(self.val)), shuffle=False)\n",
    "        print(\"Validation dataloader created:\", dl)\n",
    "        return dl\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=min(self.batch_size, len(self.test)), shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.ensemble, batch_size=min(self.batch_size, len(self.ensemble)), shuffle=False)\n",
    "\n",
    "\n",
    "class PretrainDataModule(BaseDataModule):\n",
    "    def __init__(self, filename, batch_size):\n",
    "        super().__init__(batch_size)\n",
    "        ds = PretrainDataset(filename)\n",
    "        self.train, self.val, self.test = torch.utils.data.dataset.random_split(ds, [0.4, 0.3, 0.3])\n",
    "\n",
    "\n",
    "class MCHR1DataModule(BaseDataModule):\n",
    "    def __init__(self, filename, batch_size):\n",
    "        super().__init__(batch_size)\n",
    "\n",
    "        self.train = MCHR1Dataset(filename, \"train\")\n",
    "        self.val = MCHR1Dataset(filename, \"val\")\n",
    "        self.ensemble = MCHR1Dataset(filename, \"ensemble\")\n",
    "        self.test = MCHR1Dataset(filename, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3299c1d-9482-4d5d-a717-8e7c5cd8f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold2split(fold_name, fold_id):\n",
    "    fold_nr = int(fold_name[-1])\n",
    "    if fold_nr == 8:\n",
    "        return \"ensemble\"\n",
    "    if fold_nr == 9:\n",
    "        return \"test\"\n",
    "    if fold_nr == (fold_id + 6) % 7:\n",
    "        return \"val\"\n",
    "    return \"train\"\n",
    "\n",
    "\n",
    "def prep_mchr1(output_path, fold_id):\n",
    "    df = pd.read_csv(Path(\"..\") / \"Reference Data\" / \"20240430_MCHR1_splitted_RJ.csv\")\n",
    "    \n",
    "    with open(Path(\"..\") / \"Protein Structures\" / \"Q99705.fasta\") as f:\n",
    "        tar_map = {\"P1\": \"\".join(l.strip() for l in f.readlines()[1:])}\n",
    "    lig_map = dict(df[[\"ID\", \"smiles\"]].values)\n",
    "    \n",
    "    df[\"GENE_ID\"] = \"P1\"\n",
    "    df.rename(columns={\"ID\": \"LIG_ID\"}, inplace=True)\n",
    "    df[\"split\"] = df[\"DataSAIL_10f\"].apply(lambda x: fold2split(x, fold_id))\n",
    "    inter = df[[\"LIG_ID\", \"GENE_ID\", \"acvalue_uM\", \"acname\", \"class\", \"split\"]]\n",
    "    \n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump((inter, lig_map, tar_map), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bd148fa-326d-4cce-96d3-c11d3ecc1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time2hash(npos):\n",
    "    # Get the current timestamp\n",
    "    current_timestamp = str(time.time())\n",
    "    \n",
    "    # Create a SHA-256 hash of the current timestamp\n",
    "    hash_object = hashlib.sha256(current_timestamp.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    \n",
    "    # Extract the first 4 digits of the hash\n",
    "    return hash_hex[:npos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d9a7cb-e9de-421d-9180-9ab54e920021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjo21/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce579fdec55a49d4a74c3a835670285b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataloader created: <torch_geometric.loader.dataloader.DataLoader object at 0x7f107a1d8670>\n",
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "Logging validation end\n",
      "Logging for reg\n",
      "{'reg/val/MeanAbsoluteError': tensor(3.5759, device='cuda:0'), 'reg/val/MeanSquaredError': tensor(27.0711, device='cuda:0'), 'reg/val/ExplainedVariance': tensor(0.5000, device='cuda:0')}\n",
      "Logging reg/val/MeanAbsoluteError : tensor(3.5759, device='cuda:0')\n",
      "Logging reg/val/MeanSquaredError : tensor(27.0711, device='cuda:0')\n",
      "Logging reg/val/ExplainedVariance : tensor(0.5000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjo21/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763521919bc24c20a570691b0c17be7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdd0580169940c68ae911b59b1d3290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "Logging validation end\n",
      "Logging for reg\n",
      "{'reg/val/MeanAbsoluteError': tensor(3.4954, device='cuda:0'), 'reg/val/MeanSquaredError': tensor(25.4450, device='cuda:0'), 'reg/val/ExplainedVariance': tensor(0.5004, device='cuda:0')}\n",
      "Logging reg/val/MeanAbsoluteError : tensor(3.4954, device='cuda:0')\n",
      "Logging reg/val/MeanSquaredError : tensor(25.4450, device='cuda:0')\n",
      "Logging reg/val/ExplainedVariance : tensor(0.5004, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c969fba1b24a8ebc839e02980238c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "validation step\n",
      "MetricCollection(\n",
      "  (MeanAbsoluteError): MeanAbsoluteError()\n",
      "  (MeanSquaredError): MeanSquaredError()\n",
      "  (ExplainedVariance): ExplainedVariance(),\n",
      "  prefix=reg/val/\n",
      ")\n",
      "Logging validation end\n",
      "Logging for reg\n",
      "{'reg/val/MeanAbsoluteError': tensor(3.3941, device='cuda:0'), 'reg/val/MeanSquaredError': tensor(24.0475, device='cuda:0'), 'reg/val/ExplainedVariance': tensor(0.5012, device='cuda:0')}\n",
      "Logging reg/val/MeanAbsoluteError : tensor(3.3941, device='cuda:0')\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You called `self.log(reg/val/MeanAbsoluteError, ...)` twice in `on_validation_epoch_end` with different arguments. This is not allowed",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMisconfigurationException\u001B[0m                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 62\u001B[0m\n\u001B[1;32m     58\u001B[0m     mchr1 \u001B[38;5;241m=\u001B[39m MCHR1DataModule(mchr1_path, batch_size)\n\u001B[1;32m     61\u001B[0m bs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 62\u001B[0m ckpt \u001B[38;5;241m=\u001B[39m \u001B[43mpretrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# train(ckpt, bs)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[18], line 49\u001B[0m, in \u001B[0;36mpretrain\u001B[0;34m(batch_size)\u001B[0m\n\u001B[1;32m     33\u001B[0m run_name \u001B[38;5;241m=\u001B[39m logger\u001B[38;5;241m.\u001B[39mexperiment\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     35\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     36\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     37\u001B[0m         ModelCheckpoint(save_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval/reg/loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, save_top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;66;03m# batch_size=batch_size,\u001B[39;00m\n\u001B[1;32m     47\u001B[0m )\n\u001B[0;32m---> 49\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrain_module\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPretraining on\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfinished\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     51\u001B[0m run_name \u001B[38;5;241m=\u001B[39m logger\u001B[38;5;241m.\u001B[39mexperiment\u001B[38;5;241m.\u001B[39mpath\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    576\u001B[0m     ckpt_path,\n\u001B[1;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    579\u001B[0m )\n\u001B[0;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 987\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    990\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    992\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1031\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1033\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:141\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(data_fetcher)\n\u001B[0;32m--> 141\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_advance_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.on_advance_end\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_accumulate():\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001B[39;00m\n\u001B[1;32m    293\u001B[0m     call\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_validation_model_zero_grad\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39m_first_loop_iter \u001B[38;5;241m=\u001B[39m first_loop_iter\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:142\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_store_dataloader_outputs()\n\u001B[0;32m--> 142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_run_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:254\u001B[0m, in \u001B[0;36m_EvaluationLoop.on_run_end\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39m_evaluation_epoch_end()\n\u001B[1;32m    253\u001B[0m \u001B[38;5;66;03m# hook\u001B[39;00m\n\u001B[0;32m--> 254\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_evaluation_epoch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m logged_outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logged_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logged_outputs, []  \u001B[38;5;66;03m# free memory\u001B[39;00m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;66;03m# include any logged outputs on epoch_end\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:334\u001B[0m, in \u001B[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    332\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_test_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_validation_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    333\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(trainer, hook_name)\n\u001B[0;32m--> 334\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    336\u001B[0m trainer\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39mon_epoch_end()\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:157\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    154\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 157\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    160\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "Cell \u001B[0;32mIn[12], line 128\u001B[0m, in \u001B[0;36mCACHE_DTI.on_validation_epoch_end\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_validation_epoch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogging validation end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshared_end\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 119\u001B[0m, in \u001B[0;36mCACHE_DTI.shared_end\u001B[0;34m(self, stage)\u001B[0m\n\u001B[1;32m    117\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics[task][stage]\u001B[38;5;241m.\u001B[39mcompute()\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics[task][stage]\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m--> 119\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 112\u001B[0m, in \u001B[0;36mCACHE_DTI.log_all\u001B[0;34m(self, metrics, hparams)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m metrics\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogging\u001B[39m\u001B[38;5;124m\"\u001B[39m, k, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m, v)\n\u001B[0;32m--> 112\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/core/module.py:516\u001B[0m, in \u001B[0;36mLightningModule.log\u001B[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001B[0m\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m logger \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;66;03m# we could set false here if there's no configured logger, however, we still need to compute the \"logged\"\u001B[39;00m\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;66;03m# metrics anyway because that's what the evaluation loops use as return value\u001B[39;00m\n\u001B[1;32m    514\u001B[0m     logger \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m \u001B[43mresults\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_current_fx_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprog_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprog_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon_step\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreduce_fx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreduce_fx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m    525\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_dataloader_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_dataloader_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m    \u001B[49m\u001B[43msync_dist\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msync_dist\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_accelerator_connector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_distributed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[43m    \u001B[49m\u001B[43msync_dist_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m    \u001B[49m\u001B[43msync_dist_group\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msync_dist_group\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_attribute\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_attribute\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrank_zero_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrank_zero_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    535\u001B[0m trainer\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39m_current_fx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_fx_name\n",
      "File \u001B[0;32m~/miniconda3/envs/cache_train/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:410\u001B[0m, in \u001B[0;36m_ResultCollection.log\u001B[0;34m(self, fx, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_fn, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# check the stored metadata and the current one match\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m meta \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m[key]\u001B[38;5;241m.\u001B[39mmeta:\n\u001B[0;32m--> 410\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\n\u001B[1;32m    411\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou called `self.log(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, ...)` twice in `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` with different arguments. This is not allowed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    412\u001B[0m     )\n\u001B[1;32m    414\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extract_batch_size(\u001B[38;5;28mself\u001B[39m[key], batch_size, meta)\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_metrics(key, value, batch_size)\n",
      "\u001B[0;31mMisconfigurationException\u001B[0m: You called `self.log(reg/val/MeanAbsoluteError, ...)` twice in `on_validation_epoch_end` with different arguments. This is not allowed"
     ]
    }
   ],
   "source": [
    "def pretrain(batch_size: int = 2):\n",
    "    base_path = Path(\"..\") / \"Pretrain_Data\"\n",
    "    # bdb_path = base_path / \"bdb_muts.pkl\"\n",
    "    # glass_path = base_path / \"glass.pkl\"\n",
    "    bdb_path = Path(\"data\") / \"dummy.pkl\"\n",
    "    glass_path = Path(\"data\") / \"dummy.pkl\"\n",
    "\n",
    "    bdb = PretrainDataModule(bdb_path, batch_size)\n",
    "    glass = PretrainDataModule(glass_path, batch_size)\n",
    "    run_name = None\n",
    "\n",
    "    for pretrain_module, name in [\n",
    "        (bdb, \"BindingDB\"), \n",
    "        # (glass, \"GLASS\"), \n",
    "    ]:\n",
    "\n",
    "        if run_name is None:\n",
    "            model = CACHE_DTI(batch_size)\n",
    "        else:\n",
    "            # load model\n",
    "            run = wandb.init(project=\"CACHE5\")\n",
    "            artifact = run.use_artifact(f\"rindti/CACHE5/model-{run_name}:best\", type=\"model\")\n",
    "            artifact_dir = artifact.download()\n",
    "            model = CACHE_DTI.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\")\n",
    "            wandb.finish()\n",
    "        \n",
    "        logger = WandbLogger(\n",
    "            log_model=\"all\",\n",
    "            project=\"CACHE5\",\n",
    "            # entity=\"old-shatterhand\",\n",
    "            name=name.lower() + \"_\" + time2hash(4),\n",
    "        )\n",
    "        run_name = logger.experiment.path.split(\"/\")[-1]\n",
    "\n",
    "        trainer = Trainer(\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(save_last=True, mode=\"min\", monitor=\"val/reg/loss\", save_top_k=1),\n",
    "                # RichModelSummary(),\n",
    "                # RichProgressBar(),\n",
    "            ],\n",
    "            logger=logger,\n",
    "            log_every_n_steps=1,\n",
    "            enable_model_summary=False,\n",
    "            # gpus=1,\n",
    "            max_epochs=10,\n",
    "            # batch_size=batch_size,\n",
    "        )\n",
    "            \n",
    "        trainer.fit(model, pretrain_module)\n",
    "        print(\"Pretraining on\", name, \"finished\")\n",
    "        run_name = logger.experiment.path\n",
    "    return run_name\n",
    "\n",
    "\n",
    "def train(ckpt_path, batch_size):\n",
    "    mchr1_path = Path(\"data\") / \"mchr1.pkl\"\n",
    "    prep_mchr1(mchr1_path, fold_id=0)\n",
    "    mchr1 = MCHR1DataModule(mchr1_path, batch_size)\n",
    "\n",
    "\n",
    "bs = 1\n",
    "ckpt = pretrain(bs)\n",
    "# train(ckpt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb8abb-945a-4050-a2ec-020490716f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4cc47a-4d0a-4567-bb2e-90facc48fba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e1e57-4910-4e37-bcb2-3f5cf992adf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3995aa4-020a-4e79-9d4f-5e51ef8bb399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3ad22-5fc1-4d7e-91cb-98c4c7dc93d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724854b1-4c07-496e-b17f-6dabaf6d5eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462a61e-cca6-4aec-871f-f90fc2cc43d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc5654-604b-4697-b8ab-00454aff938e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a6629-3b51-4427-ae08-c0080459e13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
