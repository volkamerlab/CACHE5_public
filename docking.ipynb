{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Used DockM8 (https://github.com/DrugBud-Suite/DockM8) to perform the Docking to the the MCHR1 receptor.\n",
    "\n",
    "The sequence was obtained from UniProt.\n",
    "\n",
    "The structure was modelled in the inactive form using AlphaFold-Multistate (https://github.com/huhlim/alphafold-multistate)\n",
    "\n",
    "The structure was minimized using a custom OpenMM script (provided)\n",
    "\n",
    "10 conformers were generated with AlphaFlow (https://github.com/bjing2016/alphaflow)\n",
    "\n",
    "The 3 most different conformers were chosen based on RMSD.\n",
    "\n",
    "Each of the conformers were used for the docking using the conditions described below. The docking library used was the top 100K predictions from our ML-ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/miniconda/envs/dockm8/lib/python3.10/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import modules for docking, scoring, protein and ligand preparation, etc.\n",
    "from scripts.clustering_functions import *\n",
    "from scripts.consensus_methods import *\n",
    "from scripts.docking_functions import *\n",
    "from scripts.dogsitescorer import *\n",
    "from scripts.get_pocket import *\n",
    "from scripts.library_preparation import *\n",
    "from scripts.performance_calculation import *\n",
    "from scripts.postprocessing import *\n",
    "from scripts.protein_preparation import *\n",
    "from scripts.rescoring_functions import *\n",
    "from scripts.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pocket_coordinates(pocket_arg):\n",
    "    try:\n",
    "        pocket_str = pocket_arg.split('*')\n",
    "        pocket_coordinates = {}\n",
    "        for item in pocket_str:\n",
    "            key, value = item.split(':')\n",
    "            pocket_coordinates[key] = list(map(float, value.split(',')))\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing pocket coordinates: {e}. Make sure the pocket coordinates are in the format 'center:1,2,3*size:1,2,3'\")\n",
    "        pocket_coordinates = None\n",
    "    return pocket_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dockm8(software, receptor, pocket, ref, docking_library, idcolumn, prepare_proteins, conformers, protonation, docking_programs, bust_poses, pose_selection, nposes, exhaustiveness, ncpus, clustering_method, rescoring, consensus):\n",
    "    # Set working directory based on the receptor file\n",
    "    w_dir = Path('/home/tony/CACHE5/SBVS/Docking/Merged/') / Path(receptor).stem\n",
    "    print('The working directory has been set to:', w_dir)\n",
    "    (w_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare the protein for docking (e.g., adding hydrogens)\n",
    "    if prepare_proteins == True:\n",
    "        prepared_receptor = Path(prepare_protein_protoss(receptor))\n",
    "    else:\n",
    "        prepared_receptor = Path(receptor)\n",
    "    \n",
    "    # Determine the docking pocket\n",
    "    if pocket == 'Reference':\n",
    "        pocket_definition = get_pocket(Path(ref), prepared_receptor, 10)\n",
    "    elif pocket == 'RoG':\n",
    "        pocket_definition = get_pocket_RoG(Path(ref), prepared_receptor)\n",
    "    elif pocket == 'Dogsitescorer':\n",
    "        pocket_definition = binding_site_coordinates_dogsitescorer(prepared_receptor, w_dir, method='volume')\n",
    "    else:\n",
    "        pocket_definition = parse_pocket_coordinates(pocket)\n",
    "    \n",
    "    print(\"The pocket coordinates are:\", pocket_definition)\n",
    "        \n",
    "    # Prepare the docking library if not already prepared\n",
    "    if not os.path.isfile(w_dir / 'final_library.sdf'):\n",
    "        prepare_library(docking_library, w_dir, idcolumn, conformers, protonation, software, ncpus)\n",
    "    \n",
    "    # Perform the docking operation\n",
    "    docking(w_dir, prepared_receptor, pocket_definition, software, docking_programs, exhaustiveness, nposes, ncpus, 'concurrent_process')\n",
    "    \n",
    "    # Concatenate all poses into a single file\n",
    "    concat_all_poses(w_dir, docking_programs, prepared_receptor, ncpus, bust_poses)\n",
    "    \n",
    "    # Load all poses from SDF file and perform clustering\n",
    "    print('Loading all poses SDF file...')\n",
    "    tic = time.perf_counter()\n",
    "    all_poses = PandasTools.LoadSDF(str(w_dir / 'allposes.sdf'), idName='Pose ID', molColName='Molecule', includeFingerprints=False, strictParsing=True)\n",
    "    toc = time.perf_counter()\n",
    "    print(f'Finished loading all poses SDF in {toc-tic:0.4f}!')\n",
    "    for method in pose_selection:\n",
    "        if not os.path.isfile(w_dir / f'clustering/{method}_clustered.sdf'):\n",
    "            select_poses(method, clustering_method, w_dir, prepared_receptor, pocket_definition, software, all_poses, ncpus)\n",
    "    \n",
    "    # Rescore poses for each selection method\n",
    "    for method in pose_selection:\n",
    "        rescore_poses(w_dir, prepared_receptor, pocket_definition, software, w_dir / 'clustering' / f'{method}_clustered.sdf', rescoring, ncpus)\n",
    "    \n",
    "    # Apply consensus methods to the poses\n",
    "    for method in pose_selection:\n",
    "        apply_consensus_methods(w_dir, method, consensus, rescoring, standardization_type='min_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_confs = [Path('/home/tony/CACHE5/SBVS/Docking/Conf1_fixed.pdb'),\n",
    "\t\t\t\tPath('/home/tony/CACHE5/SBVS/Docking/Conf2_fixed.pdb'),\n",
    "\t\t\t\t Path('/home/tony/CACHE5/SBVS/Docking/Conf3_fixed.pdb')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "for file in sorted(glob.glob('./CACHE5/SBVS/Docking/*.sdf')):\n",
    "\tos.mkdir('./CACHE5/SBVS/Docking/' + Path(file).stem) if not os.path.exists('./CACHE5/SBVS/Docking/' + Path(file).stem) else None\n",
    "\tfor conf in protein_confs:\n",
    "\t\tdockm8(software=Path('./DockM8_v1/software'),\n",
    "\t\t\treceptor=Path(conf),\n",
    "\t\t\tpocket='center:-10.4,-4.6, 5.3*size:25,20,25',\n",
    "\t\t\tref=None,\n",
    "\t\t\tdocking_library=Path(file),\n",
    "\t\t\tidcolumn='ID',\n",
    "\t\t\tprepare_proteins=True,\n",
    "\t\t\tconformers='GypsumDL',\n",
    "\t\t\tprotonation='GypsumDL',\n",
    "\t\t\tdocking_programs=['PLANTS'],\n",
    "\t\t\tbust_poses=False,\n",
    "\t\t\tpose_selection=['KORP-PL'],\n",
    "\t\t\tnposes=10,\n",
    "\t\t\texhaustiveness=16,\n",
    "\t\t\tncpus=32,\n",
    "\t\t\tclustering_method=None,\n",
    "\t\t\trescoring=['CNN-Score', 'RTMScore', 'KORP-PL'],\n",
    "\t\t\tconsensus=['RbR_best']\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "\n",
    "def merge_sdf_files(sdf_files):\n",
    "    \"\"\"Merge multiple SDF files into a single DataFrame.\"\"\"\n",
    "    frames = []\n",
    "    for file in sdf_files:\n",
    "        sdf = PandasTools.LoadSDF(file, molColName='Molecule', idName='ID', includeFingerprints=False, strictParsing=False)\n",
    "        frames.append(sdf)\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "def merge_csv_files(csv_files):\n",
    "    \"\"\"Merge multiple CSV files into a single DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# Define the source and target directories\n",
    "source_root = '/home/tony/CACHE5/SBVS/Docking/All'  # Update this path\n",
    "target_root = '/home/tony/CACHE5/SBVS/Docking/Merged'  # Update this path\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "if not os.path.exists(target_root):\n",
    "    os.makedirs(target_root)\n",
    "\n",
    "# Loop over each conformer configuration\n",
    "for conf in ['Conf1_fixed', 'Conf2_fixed', 'Conf3_fixed']:\n",
    "    sdf_files = []\n",
    "    csv_files = []\n",
    "\n",
    "    # Collect files from each chunk\n",
    "    for i in range(1, 101):  # Assuming 100 output_chunks\n",
    "        consensus_path = os.path.join(source_root, f'output_chunk_{i}', conf, 'final_library.sdf')\n",
    "        csv_path = os.path.join(source_root, f'output_chunk_{i}', conf, 'rescoring_KORP-PL_clustered', 'KORP-PL_rescoring', 'KORP-PL_scores.csv')\n",
    "        \n",
    "        if os.path.isfile(consensus_path):\n",
    "            sdf_files.append(consensus_path)\n",
    "        if os.path.isfile(csv_path):\n",
    "            csv_files.append(csv_path)\n",
    "\n",
    "    # Merge SDF files\n",
    "    if sdf_files:\n",
    "        merged_sdf = merge_sdf_files(sdf_files)\n",
    "        # Save the merged SDF\n",
    "        sdf_output_path = os.path.join(target_root, conf, 'final_library.sdf')\n",
    "        os.makedirs(os.path.dirname(sdf_output_path), exist_ok=True)\n",
    "        PandasTools.WriteSDF(merged_sdf, sdf_output_path, molColName='Molecule', idName='ID', properties=list(merged_sdf.columns))\n",
    "\n",
    "    # Merge CSV files\n",
    "    if csv_files:\n",
    "        merged_csv = merge_csv_files(csv_files)\n",
    "        # Save the merged CSV\n",
    "        csv_output_path = os.path.join(target_root, conf, 'rescoring_KORP-PL_clustered', 'KORP-PL_rescoring', 'KORP-PL_scores.csv')\n",
    "        os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
    "        merged_csv.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(\"Merging complete. Files have been saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dockm8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
